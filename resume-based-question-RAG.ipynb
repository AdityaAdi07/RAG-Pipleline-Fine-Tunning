{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5260959-3f7d-4a96-97d4-6b4c4795b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfminer.six) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from pdfminer.six) (45.0.5)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 3.7/5.6 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 16.3 MB/s  0:00:00\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20250506\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab893091-d11c-4c34-b2c8-9e0b4c8068e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aditya_CV_Latex-1\n",
      "‚úÖ Raw Text Extracted Successfully (First 500 chars):\n",
      "--------------------------------------------------\n",
      "Aditya Ankanath TR\n",
      "Bengaluru, Karnataka 560059\n",
      "(cid:131) 7760012484 # sushmaaditya717@gmail.com (cid:239) linkedin (cid:128) portfolio ¬ß github\n",
      "\n",
      "Education\n",
      "R V College of Engineering\n",
      "BE in Information Science and Engineering\n",
      "CGPA: 9.14\n",
      "\n",
      "Vidhya Vardhaka Golden Jubilee College\n",
      "PUC - PCMC\n",
      "Percentage: 94.8\n",
      "\n",
      "Relevant Coursework\n",
      "\n",
      "Aug 2023 ‚Äì 2027\n",
      "Bengaluru, Karnataka\n",
      "\n",
      "Sep 2021 ‚Äì March 2023\n",
      "Mysuru, Karnataka\n",
      "\n",
      "‚Ä¢ Data Structures\n",
      "‚Ä¢ Algorithms Analysis\n",
      "\n",
      "‚Ä¢ Data Science\n",
      "‚Ä¢ Web Development\n",
      "\n",
      "‚Ä¢ Operating Systems\n",
      "‚Ä¢...\n"
     ]
    }
   ],
   "source": [
    "# !pip install pdfminer.six\n",
    "# If you are in a Jupyter Notebook, you can run the above line by removing the initial '#' and running the cell.\n",
    "\n",
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "PDF_PATH = 'resumes/Aditya_CV_Latex-1.pdf'\n",
    "\n",
    "# Create a Path object and access its .stem attribute\n",
    "file_stem = Path(PDF_PATH).stem\n",
    "\n",
    "print(file_stem)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts raw text from a PDF file using pdfminer.six.\"\"\"\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# 1. Extract Text\n",
    "raw_text = extract_text_from_pdf(PDF_PATH)\n",
    "\n",
    "if raw_text:\n",
    "    print(\"‚úÖ Raw Text Extracted Successfully (First 500 chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(raw_text[:500] + \"...\")\n",
    "else:\n",
    "    print(\"‚ùå Could not extract text. Please check the PDF path.\")\n",
    "\n",
    "# You should replace 'your_resume.pdf' with a real file and ensure it is in the same directory, \n",
    "# or use the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d06936d-a9d4-4017-9470-a76ae48b0b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ Structured Resume Data (JSON Format):\n",
      "==================================================\n",
      "{\n",
      "    \"PERSONAL_INFO\": \"Aditya Ankanath TR\\nBengaluru, Karnataka 560059\\n(cid:131) 7760012484 # sushmaaditya717@gmail.com (cid:239) linkedin (cid:128) portfolio \\u00a7 github\",\n",
      "    \"EDUCATION\": \"R V College of Engineering\\nBE in Information Science and Engineering\\nCGPA: 9.14\\nVidhya Vardhaka Golden Jubilee College\\nPUC - PCMC\\nPercentage: 94.8\\nRelevant Coursework\\nAug 2023 \\u2013 2027\\nBengaluru, Karnataka\\nSep 2021 \\u2013 March 2023\\nMysuru, Karnataka\\n\\u2022 Data Structures\\n\\u2022 Algorithms Analysis\\n\\u2022 Data Science\\n\\u2022 Web Development\\n\\u2022 Operating Systems\\n\\u2022 Computer Networks\",\n",
      "    \"PROJECTS\": \"Plant-Disease-Detection-ML(website) | Flask, TensorFlow, MongoDB, OpenCV, JavaScript\\nJuly 2024, SIH\\n\\u2022 Developed a responsive AI web app for real-time plant disease detection using a CNN (TensorFlow) with 93%\\ntest accuracy across 38+ crop disease classes from PlantVillage dataset\\n\\u2022 MongoDB to store and retrieve over 500+ diagnosis records timestamped comparisons, tracking\\n\\u2022 Enabled automated PDF report generation (via ReportLab) with dynamic disease insights and treatment plans\\nfor 30+ diseases, reducing manual effort by 90%.\\nDisaster-News-Auth-Validate: | Flask, JavaScript, Chart.js, Weather API, Graphs, IoT Sensors. April 2025\\n\\u2022 A real-time disaster monitoring system IoT sensors and social media scraping for accurate event log.\\n\\u2022 Designed multi-role dashboards with live alerts AI-powered report verification, reduce false alerts by 40%.\\n\\u2022 Implemented graph-based simulations (Dijkstra, BFS) for emergency routing, improving efficiency by 25%.\\nBudget-Optimizer(website) | Python, JavaScript, REST APIs, ML Models, Dynamic Programming. Dec 2024\\n\\u2022 Developed a smart financial planning web app using ML models, achieving 30% improvement in budgeting\\naccuracy.\\n\\u2022 Implemented greedy and dynamic programming algorithms to optimize expense allocation and EMI planning\\nacross 50+ financial scenarios.\\nTechnical Skills\\nLanguages: Python, C, C++, HTML/CSS, JavaScript, R-Language, Matlab\\nBackend Technologies: Node, Flask, React, Next\\nFrameworks: Linux, GitHub, Power BI, MongoDB, Firebase/Supabase\",\n",
      "    \"EXPERIENCE\": \"Co-Founder (Startup Incubated by CoCreate Ventures)PES University \\u2014 Bengaluru\\nFeb 2025-Present\\n\\u2022 Selected by CoCreate Ventures LLP and PESU Venture Labs for venture studio support under a formal MOU.\\n\\u2022 Preparing to build and commercialize a tech product with equity-based engagement and structured venture.\\nFrequency-Club Website- RVCE, Bengaluru (website)\\nClub Member \\u2013 RVCE\\nApril 2025\\n\\u2022 Designed and developed the official website for Frequency Club to showcase events, team, and updates using\\nHTML, CSS, and JavaScript. Delivered a responsive, clean UI with smooth navigation.\\nAchievements\\n\\u2022 Pitch-Fest \\u2013 PES University, Bengaluru(Certification) \\u2013 Winners,1st Place\\n\\u2022 The Great Bengaluru Hackathon \\u2013 RVCE, Bengaluru(Certification) \\u2013 Top 10 of 250+\\nJan 2025\\nMar 2025\"\n",
      "}\n",
      "\n",
      "==================================================\n",
      "‚úÖ Stored in Variables (Example):\n",
      "==================================================\n",
      "Personal Info Snippet: Aditya Ankanath TR\n",
      "Bengaluru, Karnataka 560059\n",
      "(ci...\n",
      "Education Snippet: R V College of Engineering\n",
      "BE in Information Scien...\n"
     ]
    }
   ],
   "source": [
    "# Define common section headers for segmentation\n",
    "# These headers are often in ALL CAPS or Title Case and followed by the section content.\n",
    "# We'll use a regex pattern to find them.\n",
    "SECTION_TITLES = [\n",
    "    'PERSONAL INFO', 'CONTACT', 'EDUCATION', 'EXPERIENCE', 'PROJECTS', \n",
    "    'SKILLS', 'PROGRAMMING LANGUAGES', 'COURSEWORK', 'AWARDS', \n",
    "    'CERTIFICATIONS'\n",
    "]\n",
    "# Create a robust regex pattern to find any of the titles, ignoring case, \n",
    "# and potentially including a newline or a lot of space after the header.\n",
    "SECTION_REGEX = '|'.join(SECTION_TITLES)\n",
    "# The regex will find one of the section titles.\n",
    "# re.IGNORECASE makes the matching case-insensitive.\n",
    "# re.MULTILINE is helpful for text processing.\n",
    "\n",
    "def segment_resume_sections(text, section_titles):\n",
    "    \"\"\"\n",
    "    Segments the raw resume text into sections based on predefined titles.\n",
    "    This is a heuristic approach and works best if the resume has clear,\n",
    "    consistent section headings.\n",
    "    \"\"\"\n",
    "    # Normalize text: replace multiple newlines with a unique delimiter for easier splitting\n",
    "    text = re.sub(r'\\s*\\n\\s*', '\\n', text).strip()\n",
    "    \n",
    "    # 1. Split the text by section headers\n",
    "    # The 're.split' function will split the string wherever the regex matches.\n",
    "    # The parentheses around the regex make the matched separators (the section titles)\n",
    "    # included in the list.\n",
    "    pattern = r'\\n(' + '|'.join(section_titles) + r')\\n'\n",
    "    segments = re.split(pattern, text, flags=re.IGNORECASE)\n",
    "    \n",
    "    if not segments:\n",
    "        return {\"Unparsed_Content\": text}\n",
    "\n",
    "    # 2. Re-combine headers with their content\n",
    "    structured_data = {}\n",
    "    \n",
    "    # The first segment is often the name/contact info before the first section header\n",
    "    # or empty if the first segment is a header.\n",
    "    current_section = \"PERSONAL_INFO\"\n",
    "    \n",
    "    # Clean up the initial segment and treat it as the first section's content\n",
    "    initial_content = segments[0].strip()\n",
    "    if initial_content:\n",
    "        structured_data[current_section] = initial_content\n",
    "\n",
    "    # Iterate through the rest of the segments (which come in pairs of (Header, Content))\n",
    "    for i in range(1, len(segments), 2):\n",
    "        if i + 1 < len(segments):\n",
    "            # The current segment is the header (e.g., 'EDUCATION')\n",
    "            # The next segment is the content for that header.\n",
    "            header = segments[i].strip().upper().replace(' ', '_')\n",
    "            content = segments[i+1].strip()\n",
    "            \n",
    "            # Map common headers to the requested variable names\n",
    "            if 'PROGRAMMING_LANGUAGES' in header or 'SKILLS' in header:\n",
    "                current_section = 'SKILLS_OR_LANGUAGES'\n",
    "            elif 'EDUCATION' in header:\n",
    "                current_section = 'EDUCATION'\n",
    "            elif 'PROJECTS' in header:\n",
    "                current_section = 'PROJECTS'\n",
    "            elif 'COURSEWORK' in header:\n",
    "                current_section = 'COURSEWORK'\n",
    "            else:\n",
    "                # Use the header name as the section key\n",
    "                current_section = header\n",
    "            \n",
    "            # Append content if the section already exists (e.g., if we map multiple headers to one key)\n",
    "            if current_section in structured_data:\n",
    "                structured_data[current_section] += \"\\n\\n\" + content\n",
    "            else:\n",
    "                structured_data[current_section] = content\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# 2. Segment Text and Store in JSON Format\n",
    "if raw_text:\n",
    "    structured_resume_data = segment_resume_sections(raw_text, SECTION_TITLES)\n",
    "\n",
    "    # 3. Print or Store the Structured Data\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Structured Resume Data (JSON Format):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(json.dumps(structured_resume_data, indent=4))\n",
    "    \n",
    "    # Example of storing sections in variables (as requested):\n",
    "    # Safe retrieval with .get() to avoid errors if a section wasn't found\n",
    "    personal_info = structured_resume_data.get('PERSONAL_INFO', structured_resume_data.get('CONTACT', ''))\n",
    "    education = structured_resume_data.get('EDUCATION', '')\n",
    "    coursework = structured_resume_data.get('COURSEWORK', '')\n",
    "    projects = structured_resume_data.get('PROJECTS', '')\n",
    "    programming_languages = structured_resume_data.get('SKILLS_OR_LANGUAGES', '')\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"‚úÖ Stored in Variables (Example):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Personal Info Snippet: {personal_info[:50]}...\")\n",
    "    print(f\"Education Snippet: {education[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d5499c-5ee9-4ab6-83c7-34755869547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ Data successfully saved to: resumes/Aditya_CV_Latex-1.pdf.json\n",
      "==================================================\n",
      "\n",
      "First 5 lines of the saved JSON file:\n",
      "{\n",
      "    \"personal_info\": \"Aditya Ankanath TR\\nBengaluru, Karnataka 560059\\n(cid:131) 7760012484 # sushmaaditya717@gmail.com (cid:239) linkedin (cid:128) portfolio \\u00a7 github\",\n",
      "    \"education\": \"R V College of Engineering\\nBE in Information Science and Engineering\\nCGPA: 9.14\\nVidhya Vardhaka Golden Jubilee College\\nPUC - PCMC\\nPercentage: 94.8\\nRelevant Coursework\\nAug 2023 \\u2013 2027\\nBengaluru, Karnataka\\nSep 2021 \\u2013 March 2023\\nMysuru, Karnataka\\n\\u2022 Data Structures\\n\\u2022 Algorithms Analysis\\n\\u2022 Data Science\\n\\u2022 Web Development\\n\\u2022 Operating Systems\\n\\u2022 Computer Networks\",\n",
      "    \"coursework\": \"R V College of Engineering\\nBE in Information Science and Engineering\\nCGPA: 9.14\\nVidhya Vardhaka Golden Jubilee College\\nPUC - PCMC\\nPercentage: 94.8\\nRelevant Coursework\\nAug 2023 \\u2013 2027\\nBengaluru, Karnataka\\nSep 2021 \\u2013 March 2023\\nMysuru, Karnataka\\n\\u2022 Data Structures\\n\\u2022 Algorithms Analysis\\n\\u2022 Data Science\\n\\u2022 Web Development\\n\\u2022 Operating Systems\\n\\u2022 Computer Networks\",\n",
      "    \"projects\": \"Plant-Disease-Detection-ML(website) | Flask, TensorFlow, MongoDB, OpenCV, JavaScript\\nJuly 2024, SIH\\n\\u2022 Developed a responsive AI web app for real-time plant disease detection using a CNN (TensorFlow) with 93%\\ntest accuracy across 38+ crop disease classes from PlantVillage dataset\\n\\u2022 MongoDB to store and retrieve over 500+ diagnosis records timestamped comparisons, tracking\\n\\u2022 Enabled automated PDF report generation (via ReportLab) with dynamic disease insights and treatment plans\\nfor 30+ diseases, reducing manual effort by 90%.\\nDisaster-News-Auth-Validate: | Flask, JavaScript, Chart.js, Weather API, Graphs, IoT Sensors. April 2025\\n\\u2022 A real-time disaster monitoring system IoT sensors and social media scraping for accurate event log.\\n\\u2022 Designed multi-role dashboards with live alerts AI-powered report verification, reduce false alerts by 40%.\\n\\u2022 Implemented graph-based simulations (Dijkstra, BFS) for emergency routing, improving efficiency by 25%.\\nBudget-Optimizer(website) | Python, JavaScript, REST APIs, ML Models, Dynamic Programming. Dec 2024\\n\\u2022 Developed a smart financial planning web app using ML models, achieving 30% improvement in budgeting\\naccuracy.\\n\\u2022 Implemented greedy and dynamic programming algorithms to optimize expense allocation and EMI planning\\nacross 50+ financial scenarios.\\nTechnical Skills\\nLanguages: Python, C, C++, HTML/CSS, JavaScript, R-Language, Matlab\\nBackend Technologies: Node, Flask, React, Next\\nFrameworks: Linux, GitHub, Power BI, MongoDB, Firebase/Supabase\",\n"
     ]
    }
   ],
   "source": [
    "RESUME_NAME = f'{PDF_PATH}'\n",
    "OUTPUT_JSON_PATH = f'{RESUME_NAME}.json'\n",
    "\n",
    "\n",
    "# --- 3. JSON SAVING FUNCTION ---\n",
    "def save_to_json(data, output_path):\n",
    "    \"\"\"Saves the structured data dictionary to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            # Use indent=4 for a human-readable format\n",
    "            json.dump(data, f, indent=4)\n",
    "        print(f\"\\n==================================================\")\n",
    "        print(f\"‚úÖ Data successfully saved to: {output_path}\")\n",
    "        print(f\"==================================================\")\n",
    "        # Optional: Display content for confirmation\n",
    "        print(\"\\nFirst 5 lines of the saved JSON file:\")\n",
    "        with open(output_path, 'r', encoding='utf-8') as f:\n",
    "            for i in range(5):\n",
    "                print(f.readline(), end='')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving JSON file: {e}\")\n",
    "\n",
    "# --- 4. EXECUTE ---\n",
    "\n",
    "# 4a. Map the segmented data to the desired keys/variables.\n",
    "# Note: Since the input data does not have a separate 'COURSEWORK' or 'SKILLS_OR_LANGUAGES' key,\n",
    "# we map them to the raw sections that contain that information.\n",
    "personal_info = structured_resume_data.get('PERSONAL_INFO', structured_resume_data.get('CONTACT', ''))\n",
    "education = structured_resume_data.get('EDUCATION', '')\n",
    "# The coursework list is inside the 'EDUCATION' block in the raw data\n",
    "coursework = structured_resume_data.get('EDUCATION', '') \n",
    "projects = structured_resume_data.get('PROJECTS', '')\n",
    "# The technical skills list is inside the 'PROJECTS' block in the raw data\n",
    "programming_languages = structured_resume_data.get('PROJECTS', '') \n",
    "\n",
    "# 4b. Create the filtered dictionary for saving.\n",
    "filtered_data_to_save = {\n",
    "    'personal_info': personal_info,\n",
    "    'education': education,\n",
    "    'coursework': coursework,\n",
    "    'projects': projects,\n",
    "    'programming_languages': programming_languages\n",
    "}\n",
    "\n",
    "# 4c. Execute the saving function with the filtered data.\n",
    "save_to_json(filtered_data_to_save, OUTPUT_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc927cd-af3b-475b-b3d9-fcbe106d68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1953555-75a2-472d-9dbd-c7788d3cf93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED           \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    About a minute ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    7 days ago            \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "596b662f-d5f4-4419-b83a-b19e567020ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client configured to connect to local Ollama server.\n",
      "‚úÖ Successfully loaded resume data from 'resumes/Aditya_CV_Latex-1.pdf.json'\n",
      "\n",
      "üî• Starting embedding generation using the OpenAI client...\n",
      "   ‚Ü™Ô∏è  Processing section: 'personal_info'...\n",
      "   ‚Ü™Ô∏è  Processing section: 'education'...\n",
      "   ‚Ü™Ô∏è  Processing section: 'coursework'...\n",
      "   ‚Ü™Ô∏è  Processing section: 'projects'...\n",
      "   ‚Ü™Ô∏è  Processing section: 'programming_languages'...\n",
      "\n",
      "üéâ Success! Your new knowledge base is saved as 'resumes/Aditya_CV_Latex-1output.json'\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# --- 2. Configure the OpenAI Client to Point to Ollama ---\n",
    "# The client will use the environment variables we set in the .env file\n",
    "# It reads OLLAMA_BASE_URL as the server address and OLLAMA_API_KEY for authentication.\n",
    "try:\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    )\n",
    "    print(\"‚úÖ OpenAI client configured to connect to local Ollama server.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to configure OpenAI client: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Define File Paths ---\n",
    "INPUT_JSON = \"resumes/Aditya_CV_Latex-1.pdf.json\"\n",
    "OUTPUT_KB_JSON = f\"resumes/{file_stem}output.json\" # Using a new name to avoid overwriting\n",
    "\n",
    "# --- 4. Load Parsed Resume Data ---\n",
    "try:\n",
    "    with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "        resume_data = json.load(f)\n",
    "    print(f\"‚úÖ Successfully loaded resume data from '{INPUT_JSON}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: The file '{INPUT_JSON}' was not found.\")\n",
    "    raise\n",
    "\n",
    "# --- 5. Generate Embeddings and Create Knowledge Base ---\n",
    "knowledge_base = []\n",
    "print(\"\\nüî• Starting embedding generation using the OpenAI client...\")\n",
    "\n",
    "for section, text in resume_data.items():\n",
    "    if not text or not text.strip():\n",
    "        print(f\"‚ö†Ô∏è  Skipping empty section: '{section}'\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"   ‚Ü™Ô∏è  Processing section: '{section}'...\")\n",
    "    \n",
    "    try:\n",
    "        # The key change is here: using client.embeddings.create\n",
    "        response = client.embeddings.create(\n",
    "            model=\"nomic-embed-text\", # The Ollama model to use\n",
    "            input=text\n",
    "        )\n",
    "        \n",
    "        # The embedding vector is located in a different place in the response object\n",
    "        embedding_vector = response.data[0].embedding\n",
    "        \n",
    "        knowledge_base.append({\n",
    "            'section': section,\n",
    "            'text': text,\n",
    "            'embedding': embedding_vector\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to generate embedding for section '{section}': {e}\")\n",
    "\n",
    "# --- 6. Save the Knowledge Base ---\n",
    "with open(OUTPUT_KB_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(knowledge_base, f, indent=4)\n",
    "\n",
    "print(f\"\\nüéâ Success! Your new knowledge base is saved as '{OUTPUT_KB_JSON}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e7bf8d8-74ff-4ce1-99bf-c90bdd62e969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-generativeai in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (1.34.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (2.177.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sushm\\appdata\\roaming\\python\\python311\\site-packages (from google-generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.49.0rc1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\llms\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0980f8c-592a-4360-b17b-10597202ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-generativeai\n",
      "Version: 0.8.5\n",
      "Summary: Google Generative AI High level API client library and tools.\n",
      "Home-page: https://github.com/google/generative-ai-python\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: C:\\ProgramData\\anaconda3\\envs\\llms\\Lib\\site-packages\n",
      "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bb16d13-5c9a-47cb-b577-9a6c8a850f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini API configured successfully.\n",
      "üìÑ Loaded resume data from: resumes\\Aditya_CV_Latex-1.pdf.json\n",
      "\n",
      "ü§ñ Calling the Gemini Flash model to generate questions... (This may take a moment)\n",
      "\n",
      "‚úÖ Success! Here are the generated questions and answers:\n",
      "\n",
      "{\n",
      "  \"interview_questions\": [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"In your Plant-Disease-Detection-ML project, you achieved 93% test accuracy using a CNN. Can you elaborate on the architecture of the CNN you designed or chose, and what steps you took in data preprocessing and augmentation to achieve this high accuracy with the PlantVillage dataset?\",\n",
      "      \"answer\": \"For the Plant-Disease-Detection-ML project, I opted for a relatively deep Convolutional Neural Network (CNN) architecture. The architecture typically involved several convolutional layers with ReLU activation, followed by max-pooling layers for downsampling. Batch normalization was included to stabilize training, and finally, fully connected layers led to an output layer with a softmax activation function to classify across the 38+ disease classes. To handle the PlantVillage dataset, which can sometimes have class imbalance or varying image conditions, I performed extensive data preprocessing including resizing, normalization, and converting images to appropriate formats. Data augmentation techniques such as random rotations, flips, zooms, and brightness adjustments were crucial to increase the diversity of the training data and prevent overfitting, significantly contributing to the 93% test accuracy.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"You used MongoDB to store over 500+ diagnosis records in your Plant-Disease-Detection-ML project. Why was MongoDB a suitable choice for this specific application, and how did you leverage its capabilities for 'timestamped comparisons, tracking'?\",\n",
      "      \"answer\": \"MongoDB was chosen for its NoSQL, document-oriented nature, which offered flexibility in storing diagnosis records. Each record could have varying fields, such as image metadata, detected disease, confidence score, and treatment plan details, without needing a rigid schema. This was ideal as the data structure might evolve. For timestamped comparisons and tracking, I embedded a `timestamp` field in each diagnosis document. MongoDB's indexing capabilities allowed for efficient querying based on date ranges to compare diagnoses over time, track the progression of plant health for a specific user, or analyze overall diagnosis trends. Aggregation pipelines could then be used to generate summaries or calculate changes between different periods.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"The Plant-Disease-Detection-ML project reduced manual effort by 90% through automated PDF report generation using ReportLab. Can you describe the integration process of ReportLab within your Flask application and how you dynamically inserted disease insights and treatment plans?\",\n",
      "      \"answer\": \"Integrating ReportLab with the Flask application involved creating a dedicated endpoint in Flask that, upon request, would trigger the PDF generation. When a user requested a report, the Flask backend would retrieve the diagnosis data (including disease insights and relevant treatment plans, potentially stored in MongoDB or pre-configured in the app) corresponding to the specific detection. This data was then passed to a Python function that utilized ReportLab. ReportLab objects like `SimpleDocTemplate`, `Paragraph`, `Image`, and `Table` were used. The dynamic content, such as disease name, accuracy, insights, and treatment steps, was pulled from the diagnosis data and formatted into strings or lists, which ReportLab then rendered into the PDF. This automation significantly streamlined the process of providing comprehensive reports to users.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"In the Disaster-News-Auth-Validate system, you integrated IoT sensors and social media scraping. What were the key technical challenges in unifying these disparate data sources, and how did your 'AI-powered report verification' specifically address the issue of false alerts from social media?\",\n",
      "      \"answer\": \"The main technical challenge in unifying data from IoT sensors and social media was standardizing the data format and ensuring real-time ingestion. IoT sensor data typically arrived as structured, time-series numerical data, while social media scraping yielded unstructured text. I implemented a data pipeline where sensor data was ingested via an API or MQTT, and social media data was collected using Python scrapers (e.g., Tweepy, Beautiful Soup) and then preprocessed, including cleaning, tokenization, and sentiment analysis for relevant keywords. For 'AI-powered report verification,' I trained a machine learning model, likely a text classification model (e.g., using NLP techniques like TF-IDF or word embeddings with a Logistic Regression or SVM classifier), to assess the credibility and urgency of social media posts. The model would look for specific linguistic patterns, keywords, and cross-reference with sensor data or official sources. This helped filter out rumors, jokes, or irrelevant posts, reducing false alerts by 40%.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"You used graph-based simulations (Dijkstra, BFS) for emergency routing in the Disaster-News-Auth-Validate system, improving efficiency by 25%. Can you explain how you modeled the 'graph' for this application and provide an example of a specific scenario where Dijkstra's algorithm would be more appropriate than BFS?\",\n",
      "      \"answer\": \"For emergency routing, the 'graph' was modeled where nodes represented key locations, intersections, or safe zones, and edges represented roads or pathways connecting them. Each edge had associated weights, which could represent dynamic factors like travel time, road conditions (e.g., reported damage from IoT sensors or social media), or congestion levels. Dijkstra's algorithm is ideal for finding the shortest path in a graph with non-negative edge weights. A scenario where Dijkstra would be more appropriate than BFS is when emergency services need to find the quickest route to an incident location, considering real-time traffic, road closures, or estimated damage. BFS, which finds the shortest path in terms of the number of edges (unweighted), would be less effective here, as a route with fewer segments might take longer due if each segment has significantly different traversal times.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"In the Budget-Optimizer project, you used ML models to achieve a 30% improvement in budgeting accuracy. What specific types of ML models did you employ for financial planning, and how did you quantify and measure this 'budgeting accuracy'?\",\n",
      "      \"answer\": \"For the Budget-Optimizer, I explored and utilized regression models to predict future expenses and optimize savings. Specifically, models like Linear Regression or even more complex Ensemble methods such as Random Forests or Gradient Boosting Regressors were considered, depending on the complexity and patterns in the financial data. These models would analyze historical spending patterns, income, and recurring bills to forecast future financial inflows and outflows. 'Budgeting accuracy' was quantified by comparing the model's predicted expenses and savings against actual financial outcomes over a defined period. Metrics like Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) were used to measure the deviation between predicted and actual values. A 30% improvement would mean a significant reduction in these error metrics, indicating more reliable financial forecasts and better planning recommendations for the user.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"The Budget-Optimizer project leveraged both greedy and dynamic programming algorithms. Can you provide a practical example from the project where a greedy approach was optimal for expense allocation, and another where dynamic programming was essential for EMI planning across 50+ financial scenarios?\",\n",
      "      \"answer\": \"Certainly. A practical example where a greedy approach could be optimal for expense allocation might involve prioritizing the repayment of loans with the highest interest rates first, assuming all other factors are equal and the goal is to minimize total interest paid quickly. This is a common strategy known as the 'debt avalanche method.' However, for EMI planning across 50+ financial scenarios, dynamic programming was essential. This is because EMI planning often involves optimizing payments over a long period with varying incomes, expenses, and investment opportunities, where a locally optimal greedy choice (e.g., paying maximum EMI this month) might not lead to the globally optimal outcome (e.g., maximizing long-term savings or investment returns). Dynamic programming allowed for breaking down the problem into sub-problems, storing intermediate results, and building up to an optimal solution that considered all future constraints and goals, leading to a more comprehensive and efficient financial plan.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"You've listed Flask, Node, React, and Next in your 'Backend Technologies' section, although React and Next are primarily frontend frameworks often used for full-stack. Can you discuss when you would choose Flask over Node.js for a backend project, and conversely, a scenario where Node.js would be preferable?\",\n",
      "      \"answer\": \"My apologies for the slight miscategorization of React and Next \\u2013 I primarily use them for frontend with server-side rendering or API consumption. Regarding Flask vs. Node.js, I would typically choose Flask (Python) for projects that involve heavy data processing, machine learning integration, or scientific computing, as Python has a rich ecosystem of libraries like TensorFlow, NumPy, and Pandas. The Plant-Disease-Detection-ML project is a prime example where Flask was ideal due to its integration with TensorFlow. Conversely, I would prefer Node.js for building highly scalable, real-time applications that are I/O bound, such as chat applications, live dashboards (like in Disaster-News-Auth-Validate for real-time alerts), or streaming services. Its asynchronous, non-blocking nature makes it very efficient for handling many concurrent connections, and its JavaScript-everywhere paradigm can simplify full-stack development.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"You've worked with MongoDB and listed Firebase/Supabase. When would you typically opt for a NoSQL database like MongoDB or Firebase, and what are their specific advantages over a traditional relational database for the types of projects you've described?\",\n",
      "      \"answer\": \"I would typically opt for a NoSQL database when dealing with rapidly evolving schemas, large volumes of unstructured or semi-structured data, or when high scalability requirements dictate horizontal scaling. For example, in the Plant-Disease-Detection-ML project, MongoDB was suitable because diagnosis records might have varied metadata over time, and its document model allowed for flexible fields without complex migrations. Firebase/Supabase are excellent for projects requiring real-time data synchronization, built-in authentication, and serverless functions, accelerating development for web and mobile apps. Their advantages over relational databases include schema flexibility, easier horizontal scaling, and often a simpler mental model for certain data types. However, relational databases excel in situations requiring complex joins, strong ACID compliance, and strictly defined relationships, which might not have been the primary concern in my current projects.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 10,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"Your coursework includes Data Structures and Algorithms Analysis. Can you discuss a time when your understanding of efficient algorithms significantly impacted a design decision or problem-solving approach in one of your projects, leading to a measurable improvement?\",\n",
      "      \"answer\": \"My understanding of efficient algorithms was critical in the Disaster-News-Auth-Validate project, particularly when implementing the emergency routing system. Initially, one might consider a brute-force approach for finding paths, but with a growing network of roads and potential incident locations, this would quickly become computationally infeasible for real-time use. By applying graph-based algorithms like Dijkstra's (for weighted shortest paths) and BFS (for unweighted shortest paths), I was able to model the road network efficiently. This algorithmic choice directly led to the 25% improvement in routing efficiency because it allowed the system to quickly calculate optimal paths even with dynamic weights (e.g., considering traffic or damaged roads), which is crucial in emergency scenarios where time is of the essence. Without these efficient algorithms, the system would not have been able to provide timely and reliable routing suggestions.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"area\": \"Behavioral\",\n",
      "      \"question\": \"Reflecting on any of your projects, what was the most significant technical challenge you encountered, and how did you approach solving it?\",\n",
      "      \"answer\": \"The most significant technical challenge I faced was in the Disaster-News-Auth-Validate project, specifically around the 'AI-powered report verification' for social media data. The sheer volume and inherent noise (irrelevant posts, sarcasm, false alarms) in real-time social media streams made it incredibly difficult to accurately identify genuine disaster reports. My approach involved several steps: First, I focused on robust data preprocessing, including extensive text cleaning, removing stopwords, and performing named entity recognition to extract relevant information. Second, I experimented with various NLP models. I started with simpler models like Naive Bayes and progressively moved to more sophisticated ones, ultimately using a text classification model trained on a curated dataset of verified and unverified disaster reports. The biggest breakthrough came from combining linguistic features with contextual information from IoT sensors and weather APIs, allowing the AI to cross-reference data points and significantly reduce false alerts by 40%. It was an iterative process of model training, evaluation, and feature engineering.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"area\": \"Behavioral\",\n",
      "      \"question\": \"Your projects involve a diverse set of technologies, from TensorFlow and MongoDB to Chart.js and ReportLab. How do you approach learning a new programming language, framework, or library when it's required for a project?\",\n",
      "      \"answer\": \"When approaching a new technology, my first step is usually to consult the official documentation and look for beginner-friendly tutorials or quick-start guides. I prefer a hands-on approach, so I'll create a small, isolated proof-of-concept project to understand its core functionalities and syntax. For example, when learning ReportLab, I built a simple script to generate a basic PDF before integrating it into my Flask app. I also leverage online resources like Stack Overflow, developer communities, and relevant articles for specific problem-solving. My goal is always to understand 'why' a technology is used in a certain way, not just 'how' to use it. This iterative process of learning, implementing, debugging, and refining helps me quickly become proficient enough to integrate it effectively into my projects.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"Consider your Plant-Disease-Detection-ML project. If it were to be deployed globally and handle millions of users and diagnoses per day, what architectural changes or considerations would be most important for scalability?\",\n",
      "      \"answer\": \"For global deployment and handling millions of users, several architectural changes would be crucial. Firstly, the Flask application would need to be horizontally scaled, likely using containerization technologies like Docker and orchestration with Kubernetes, distributed across multiple regions. The MongoDB database would require sharding and replication to distribute data and read/write operations efficiently, ensuring high availability and fault tolerance. For the TensorFlow model, efficient inference is key; this could involve optimizing the model for faster prediction, potentially using TensorFlow Lite or ONNX for edge deployment, or deploying the model as a microservice on dedicated GPU instances. A Content Delivery Network (CDN) would be essential for serving static assets and the web interface globally. Message queues (e.g., RabbitMQ, Kafka) could decouple the image upload/processing from the user-facing application, allowing for asynchronous processing and better resilience under heavy load.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 14,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"In your Budget-Optimizer project, you mention using REST APIs. Can you describe your approach to designing a robust and efficient REST API for financial data, considering aspects like resource naming, authentication, and error handling?\",\n",
      "      \"answer\": \"When designing REST APIs for the Budget-Optimizer, my approach prioritized clarity, consistency, and security. For resource naming, I adhered to RESTful principles, using nouns to represent resources (e.g., `/users`, `/expenses`, `/budgets`) and HTTP verbs (GET, POST, PUT, DELETE) for actions. URLs were kept clean and intuitive. For authentication, I would typically implement token-based authentication, such as JWT (JSON Web Tokens), where users log in once to receive a token that's then sent with subsequent requests. This ensures statelessness and security. Error handling was crucial for a financial application; I designed API responses to provide clear HTTP status codes (e.g., 200 OK, 201 Created, 400 Bad Request, 401 Unauthorized, 404 Not Found, 500 Internal Server Error) along with descriptive JSON error bodies that explain what went wrong, aiding both frontend development and debugging. Versioning the API (e.g., `/v1/users`) would also be considered for future compatibility.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 15,\n",
      "      \"area\": \"Behavioral\",\n",
      "      \"question\": \"Describe a complex bug you encountered in one of your projects that took a significant amount of time to resolve. What was your systematic approach to debugging it?\",\n",
      "      \"answer\": \"In the Budget-Optimizer project, a complex bug arose where the ML model's budgeting accuracy would intermittently drop significantly for certain user profiles, without any clear pattern. My systematic approach to debugging began with **reproduction**: I tried to isolate the conditions under which the bug occurred by testing with different user data and financial scenarios. Next, I used **logging and print statements** extensively at various stages of the data processing and model inference pipeline to trace the flow of data and intermediate results. I hypothesized that it might be related to edge cases in the data, feature scaling, or a specific interaction between greedy and dynamic programming algorithms. I then used a **debugger** (e.g., `pdb` in Python) to step through the code line by line, inspecting variable states. Eventually, I discovered an issue with how missing values in certain financial categories were being handled during feature engineering for a subset of users, leading to incorrect input for the ML model. After **identifying the root cause**, I implemented a more robust imputation strategy, thoroughly **tested** the fix, and verified that the accuracy drop no longer occurred, restoring the 30% budgeting accuracy improvement.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 16,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"For the Disaster-News-Auth-Validate system, beyond the algorithm choice (Dijkstra/BFS), what other performance optimizations did you consider or implement to handle real-time data from IoT sensors and social media effectively?\",\n",
      "      \"answer\": \"Beyond algorithm choice, several other performance optimizations were critical for handling real-time data in the Disaster-News-Auth-Validate system. For IoT sensor data, I considered using lightweight protocols like MQTT for efficient message transfer and implemented efficient data parsing directly at the ingestion point to minimize latency. For social media scraping, to avoid rate limits and process data efficiently, I utilized asynchronous requests and potentially distributed scraping agents. Caching was implemented for frequently accessed data, such as verified incident reports or static map data, to reduce database load. Furthermore, I explored using message queues (like RabbitMQ) to decouple the data ingestion, processing, and alert generation components. This allows for asynchronous processing, ensures data durability, and prevents bottlenecks, making the system more resilient and responsive under varying loads of real-time incoming data.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 17,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"You listed GitHub as a 'Framework'. How do you typically use Git and GitHub in your project workflow, especially when collaborating on projects like SIH, and what Git workflow do you usually follow?\",\n",
      "      \"answer\": \"My apologies for listing GitHub as a framework; I use it primarily for version control and collaboration. In my project workflow, especially for collaborative efforts like the SIH, I typically follow a feature branch workflow. Each new feature, bug fix, or project component starts with its own dedicated branch off the `main` or `develop` branch. This allows team members to work independently without interfering with each other's code. Once a feature is complete, I create a pull request on GitHub, which triggers code reviews from teammates. We discuss changes, address feedback, and resolve any merge conflicts before merging the branch into the main development line. I prioritize clear, concise commit messages, and use `git rebase -i` to clean up commit history before merging to maintain a clean and understandable project history. GitHub's issue tracking and project boards are also instrumental for task management and planning.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 18,\n",
      "      \"area\": \"Behavioral\",\n",
      "      \"question\": \"In any of your projects, can you describe a situation where you had to make a significant design or implementation trade-off (e.g., performance vs. development time, accuracy vs. interpretability)? What was the trade-off, and how did you decide?\",\n",
      "      \"answer\": \"In the Plant-Disease-Detection-ML project, a significant trade-off involved choosing between the highest possible model accuracy and the inference speed required for a 'real-time' web app experience. Initially, I experimented with very deep and complex CNN architectures that yielded slightly higher accuracy (e.g., 94-95%). However, these models had longer inference times, leading to noticeable delays when users uploaded images on the Flask web app. The trade-off was between a marginal increase in accuracy and maintaining a responsive user experience. I decided to stick with a slightly less complex CNN architecture, which achieved 93% accuracy but significantly faster inference times. The reasoning was that for a user-facing application, a quick and smooth experience with 'very good' accuracy (93%) was more impactful than a fractional accuracy gain at the cost of perceived sluggishness. User experience and responsiveness were prioritized over marginal model performance, especially given the context of a web application.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 19,\n",
      "      \"area\": \"Projects\",\n",
      "      \"question\": \"If you had more time or resources, what are some of the next features or improvements you would implement for your Plant-Disease-Detection-ML project?\",\n",
      "      \"answer\": \"If I had more time and resources for the Plant-Disease-Detection-ML project, my top priorities for improvements would be: First, expanding the dataset and model to support a wider variety of crops and diseases, including regional specific issues, which could potentially increase the 38+ classes even further. Second, I would integrate more advanced computer vision techniques for detecting early-stage diseases that might not be as visually apparent, possibly using segmentation models alongside classification. Third, I'd consider developing a mobile application version for on-the-go diagnosis, potentially integrating edge computing for offline capabilities. Finally, I would implement user profiles to track individual plant health histories, offer personalized long-term care recommendations, and potentially build a community feature where users can share their experiences and get advice from experts or other users.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 20,\n",
      "      \"area\": \"Technical_Skills\",\n",
      "      \"question\": \"Your coursework lists 'Operating Systems' and 'Computer Networks'. How has your understanding of these subjects influenced your approach to developing web applications like your Plant-Disease-Detection-ML site or your real-time Disaster-News-Auth-Validate system?\",\n",
      "      \"answer\": \"My understanding of Operating Systems and Computer Networks has profoundly influenced my web application development. From an Operating Systems perspective, I gained insights into process management, memory management, and file systems, which helps me write more efficient code that interacts well with the underlying OS resources. For example, understanding concurrency models is key for building responsive Flask applications, and managing file I/O (like handling image uploads for Plant Disease Detection) efficiently. From a Computer Networks standpoint, I better understand how HTTP requests and responses work, the role of TCP/IP, and concepts like latency, bandwidth, and security. This knowledge informs decisions on API design (e.g., minimizing payload size for faster responses), choosing appropriate protocols (e.g., WebSocket for real-time alerts in Disaster-News), and considering security implications like cross-site scripting or data encryption for all my web projects.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "üìù Output also saved to: resumes\\interview_questions.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- 1. Load API Key and Configure Gemini ---\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"‚ùå Error: GOOGLE_API_KEY not found. Please create a .env file and add your key.\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ Gemini API configured successfully.\")\n",
    "\n",
    "# --- 2. Load Resume JSON from Folder ---\n",
    "resume_folder = \"resumes\"\n",
    "resume_filename = \"Aditya_CV_Latex-1.pdf.json\"\n",
    "resume_path = os.path.join(resume_folder, resume_filename)\n",
    "\n",
    "if not os.path.exists(resume_path):\n",
    "    raise FileNotFoundError(f\"‚ùå Resume JSON not found at: {resume_path}\")\n",
    "\n",
    "with open(resume_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    resume_data = json.load(f)\n",
    "\n",
    "print(f\"üìÑ Loaded resume data from: {resume_path}\")\n",
    "\n",
    "# --- 3. Convert Resume Data into Structured Text ---\n",
    "resume_text = \"\"\n",
    "for section, content in resume_data.items():\n",
    "    resume_text += f\"## {section.replace('_', ' ').title()}\\n{content}\\n\\n\"\n",
    "\n",
    "# --- 4. Build the Gemini Prompt ---\n",
    "prompt = f\"\"\"\n",
    "You are an expert technical interviewer preparing for an interview with a candidate named Aditya.\n",
    "Your task is to analyze the provided resume text thoroughly and generate a comprehensive list of 15 interview questions.\n",
    "\n",
    "**Instructions:**\n",
    "1. Generate a mix of technical, project-deep-dive, and behavioral questions.\n",
    "2. Base every question directly on the information given in the resume.\n",
    "3. For each question, provide a strong, detailed reference answer that the candidate might give, drawing logical conclusions from their project details.\n",
    "4. The final output MUST be a single, valid JSON object. Do not add any text before or after the JSON object.\n",
    "5. Use the following JSON structure:\n",
    "{{\n",
    "  \"interview_questions\": [\n",
    "    {{\n",
    "      \"id\": <integer>,\n",
    "      \"area\": \"<Projects/Technical_Skills/Behavioral>\",\n",
    "      \"question\": \"<The generated question>\",\n",
    "      \"answer\": \"<The detailed reference answer>\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Resume to Analyze:**\n",
    "---\n",
    "{resume_text}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. Call Gemini Model ---\n",
    "try:\n",
    "    print(\"\\nü§ñ Calling the Gemini Flash model to generate questions... (This may take a moment)\")\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        \"models/gemini-2.5-flash\",\n",
    "        generation_config=genai.types.GenerationConfig(\n",
    "            response_mime_type=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    # --- 6. Parse JSON Response ---\n",
    "    generated_json = json.loads(response.text)\n",
    "\n",
    "    print(\"\\n‚úÖ Success! Here are the generated questions and answers:\\n\")\n",
    "    print(json.dumps(generated_json, indent=2))\n",
    "\n",
    "    # --- 7. Save Output ---\n",
    "    output_filename = os.path.join(resume_folder, \"interview_questions.json\")\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(generated_json, f, indent=2)\n",
    "    print(f\"\\nüìù Output also saved to: {output_filename}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå An error occurred: {e}\")\n",
    "    print(\"   Please check your API key and ensure it is configured correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c383e02a-9a23-45d4-b9dc-d0875007b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OpenAI client configured to connect to local Ollama server.\n",
      "‚úÖ Loaded 20 questions from 'resumes/interview_questions.json'\n",
      "üîπ Generating embedding for question 1: In your Plant-Disease-Detection-ML project, you achieved 93%...\n",
      "üîπ Generating embedding for question 2: You used MongoDB to store over 500+ diagnosis records in you...\n",
      "üîπ Generating embedding for question 3: The Plant-Disease-Detection-ML project reduced manual effort...\n",
      "üîπ Generating embedding for question 4: In the Disaster-News-Auth-Validate system, you integrated Io...\n",
      "üîπ Generating embedding for question 5: You used graph-based simulations (Dijkstra, BFS) for emergen...\n",
      "üîπ Generating embedding for question 6: In the Budget-Optimizer project, you used ML models to achie...\n",
      "üîπ Generating embedding for question 7: The Budget-Optimizer project leveraged both greedy and dynam...\n",
      "üîπ Generating embedding for question 8: You've listed Flask, Node, React, and Next in your 'Backend ...\n",
      "üîπ Generating embedding for question 9: You've worked with MongoDB and listed Firebase/Supabase. Whe...\n",
      "üîπ Generating embedding for question 10: Your coursework includes Data Structures and Algorithms Anal...\n",
      "üîπ Generating embedding for question 11: Reflecting on any of your projects, what was the most signif...\n",
      "üîπ Generating embedding for question 12: Your projects involve a diverse set of technologies, from Te...\n",
      "üîπ Generating embedding for question 13: Consider your Plant-Disease-Detection-ML project. If it were...\n",
      "üîπ Generating embedding for question 14: In your Budget-Optimizer project, you mention using REST API...\n",
      "üîπ Generating embedding for question 15: Describe a complex bug you encountered in one of your projec...\n",
      "üîπ Generating embedding for question 16: For the Disaster-News-Auth-Validate system, beyond the algor...\n",
      "üîπ Generating embedding for question 17: You listed GitHub as a 'Framework'. How do you typically use...\n",
      "üîπ Generating embedding for question 18: In any of your projects, can you describe a situation where ...\n",
      "üîπ Generating embedding for question 19: If you had more time or resources, what are some of the next...\n",
      "üîπ Generating embedding for question 20: Your coursework lists 'Operating Systems' and 'Computer Netw...\n",
      "\n",
      "üéâ Success! Embeddings saved to 'resumes/interview_questions_with_embeddings.json'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "        api_key=os.getenv(\"OLLAMA_API_KEY\"),\n",
    "    )\n",
    "    print(\" OpenAI client configured to connect to local Ollama server.\")\n",
    "except Exception as e:\n",
    "    print(f\" Failed to configure OpenAI client: {e}\")\n",
    "    raise\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Define file paths\n",
    "INPUT_JSON = \"resumes/interview_questions.json\"\n",
    "OUTPUT_JSON = \"resumes/interview_questions_with_embeddings.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Load interview questions\n",
    "try:\n",
    "    with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    questions = data.get(\"interview_questions\", [])\n",
    "    print(f\"‚úÖ Loaded {len(questions)} questions from '{INPUT_JSON}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{INPUT_JSON}' not found\")\n",
    "    raise\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Generate embeddings using Ollama\n",
    "for idx, q in enumerate(questions, start=1):\n",
    "    answer_text = q.get(\"answer\", \"\")\n",
    "    if not answer_text.strip():\n",
    "        print(f\"‚ö†Ô∏è  Skipping empty answer for question {idx}\")\n",
    "        q[\"answer_embedding\"] = []\n",
    "        continue\n",
    "    \n",
    "    print(f\"üîπ Generating embedding for question {idx}: {q.get('question', '')[:60]}...\")\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            model=\"nomic-embed-text\",  # Ollama embedding model\n",
    "            input=answer_text\n",
    "        )\n",
    "        embedding_vector = response.data[0].embedding\n",
    "        q[\"answer_embedding\"] = embedding_vector\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed for question {idx}: {e}\")\n",
    "        q[\"answer_embedding\"] = []\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save updated JSON\n",
    "os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nüéâ Success! Embeddings saved to '{OUTPUT_JSON}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85833a08-f061-4982-b247-a083e64572f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 20 WAV audio files...\n",
      "\n",
      "\n",
      "All questions converted successfully and saved in: audio_questions_wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing:   0%|          | 0/20 [00:00<?, ?file/s]\n",
      "Processing:   5%|5         | 1/20 [00:00<00:15,  1.20file/s]\n",
      "Processing:  10%|#         | 2/20 [00:01<00:10,  1.74file/s]\n",
      "Processing:  15%|#5        | 3/20 [00:01<00:08,  2.02file/s]\n",
      "Processing:  20%|##        | 4/20 [00:02<00:07,  2.19file/s]\n",
      "Processing:  25%|##5       | 5/20 [00:02<00:06,  2.29file/s]\n",
      "Processing:  30%|###       | 6/20 [00:02<00:05,  2.37file/s]\n",
      "Processing:  35%|###5      | 7/20 [00:03<00:05,  2.31file/s]\n",
      "Processing:  40%|####      | 8/20 [00:03<00:05,  2.30file/s]\n",
      "Processing:  45%|####5     | 9/20 [00:04<00:04,  2.34file/s]\n",
      "Processing:  50%|#####     | 10/20 [00:04<00:04,  2.36file/s]\n",
      "Processing:  55%|#####5    | 11/20 [00:04<00:03,  2.40file/s]\n",
      "Processing:  60%|######    | 12/20 [00:05<00:03,  2.42file/s]\n",
      "Processing:  65%|######5   | 13/20 [00:05<00:02,  2.46file/s]\n",
      "Processing:  70%|#######   | 14/20 [00:06<00:02,  2.42file/s]\n",
      "Processing:  75%|#######5  | 15/20 [00:06<00:02,  2.47file/s]\n",
      "Processing:  80%|########  | 16/20 [00:07<00:01,  2.37file/s]\n",
      "Processing:  85%|########5 | 17/20 [00:07<00:01,  2.35file/s]\n",
      "Processing:  90%|######### | 18/20 [00:07<00:00,  2.38file/s]\n",
      "Processing:  95%|#########5| 19/20 [00:08<00:00,  2.45file/s]\n",
      "Processing: 100%|##########| 20/20 [00:08<00:00,  2.37file/s]\n",
      "Processing: 100%|##########| 20/20 [00:08<00:00,  2.30file/s]\n"
     ]
    }
   ],
   "source": [
    "!python generate_audio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf2c789-46d9-444d-bcbc-0cf6d7d465c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, whisper, time\n",
    "from tqdm import tqdm\n",
    "import speech_recognition as sr\n",
    "from playsound import playsound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88b1403-4b64-4a96-8cec-2584bcae6689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:11<00:00, 12.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1 questions.\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper model once (local, offline)\n",
    "model = whisper.load_model(\"base\")  # use \"tiny\" for faster performance\n",
    "\n",
    "# Create folder for answers\n",
    "os.makedirs(\"answers\", exist_ok=True)\n",
    "\n",
    "# Load interview questions\n",
    "with open(\"resumes/interview_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(questions)} questions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1459c76-4ec7-45d8-9012-79d574f719a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (small) for better transcription...\n",
      "================================================================================\n",
      "Question 1: In your Plant-Disease-Detection-ML project, you achieved 93% test accuracy using a CNN. Can you elaborate on the architecture of the CNN you designed or chose, and what steps you took in data preprocessing and augmentation to achieve this high accuracy with the PlantVillage dataset?\n",
      "\n",
      "Press Enter to start recording your answer...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording answer (max 15 seconds)...\n",
      "Recorded Answer: Hello, can you hear me? Can you hear me?\n",
      "Saved WAV: user_answers_wav\\Q1_answer.wav\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Proceed to next question? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending interview session early.\n",
      "\n",
      "Interview session completed! Answers saved to transcript/interview_answers.json\n"
     ]
    }
   ],
   "source": [
    "%run interview_session_terminal.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "746b4d52-b824-4678-97c7-f638861ec53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1\n",
      "Reference Question: In your Plant-Disease-Detection-ML project, you achieved 93% test accuracy using a CNN. Can you elaborate on the architecture of the CNN you designed or chose, and what steps you took in data preprocessing and augmentation to achieve this high accuracy with the PlantVillage dataset?\n",
      "User Answer: I used CNN Mollinger.\n",
      "Semantic Similarity: 56.39 %\n",
      "Keyword Coverage: 20.0 %\n",
      "Final Combined Score: 38.2 %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Configure Ollama/OpenAI client\n",
    "load_dotenv()\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"OLLAMA_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OLLAMA_API_KEY\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Load reference questions with embeddings\n",
    "REFERENCE_JSON = \"resumes/interview_questions_with_embeddings.json\"\n",
    "with open(REFERENCE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    reference_data = json.load(f)\n",
    "reference_questions = reference_data.get(\"interview_questions\", [])\n",
    "\n",
    "# -----------------------------\n",
    "# Load transcript answers\n",
    "TRANSCRIPT_JSON = \"transcript/interview_answers.json\"\n",
    "with open(TRANSCRIPT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    user_answers = json.load(f)\n",
    "\n",
    "if not user_answers:\n",
    "    raise ValueError(\"No answers found in transcript.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    if np.linalg.norm(vec1) == 0 or np.linalg.norm(vec2) == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))\n",
    "\n",
    "# -----------------------------\n",
    "# Extract keywords from reference answer\n",
    "def extract_keywords(text, top_n=10):\n",
    "    # Lowercase, remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    words = text.split()\n",
    "    stopwords = set([\"the\", \"and\", \"a\", \"of\", \"to\", \"in\", \"for\", \"with\", \"is\", \"on\", \"that\", \"this\", \"as\", \"it\", \"can\"])\n",
    "    keywords = [w for w in words if w not in stopwords]\n",
    "    return list(dict.fromkeys(keywords))[:top_n]  # unique top_n words\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate all answers\n",
    "results = []\n",
    "\n",
    "for idx, user_entry in enumerate(user_answers):\n",
    "    user_text = user_entry.get(\"user_answer\", \"\").strip()\n",
    "    if not user_text:\n",
    "        results.append({\"question_index\": idx+1, \"score_percent\": 0, \"note\": \"Empty answer\"})\n",
    "        continue\n",
    "    \n",
    "    # Reference embedding & answer\n",
    "    if idx >= len(reference_questions):\n",
    "        results.append({\"question_index\": idx+1, \"score_percent\": 0, \"note\": \"No reference question\"})\n",
    "        continue\n",
    "    \n",
    "    ref = reference_questions[idx]\n",
    "    ref_embedding = np.array(ref.get(\"answer_embedding\", []))\n",
    "    ref_text = ref.get(\"answer\", \"\")\n",
    "    \n",
    "    if len(ref_embedding) == 0 or not ref_text.strip():\n",
    "        results.append({\"question_index\": idx+1, \"score_percent\": 0, \"note\": \"No reference embedding or text\"})\n",
    "        continue\n",
    "    \n",
    "    # User embedding\n",
    "    response = client.embeddings.create(\n",
    "        model=\"nomic-embed-text\",\n",
    "        input=user_text\n",
    "    )\n",
    "    user_embedding = np.array(response.data[0].embedding)\n",
    "    \n",
    "    # Cosine similarity (semantic score)\n",
    "    sem_score = cosine_similarity(user_embedding, ref_embedding)\n",
    "    \n",
    "    # Keyword coverage score\n",
    "    ref_keywords = extract_keywords(ref_text)\n",
    "    user_words = set(re.sub(r\"[^\\w\\s]\", \"\", user_text.lower()).split())\n",
    "    if ref_keywords:\n",
    "        coverage = sum(1 for kw in ref_keywords if kw in user_words) / len(ref_keywords)\n",
    "    else:\n",
    "        coverage = 0.0\n",
    "    \n",
    "    # Combine scores: weighted average (50% embedding + 50% keyword coverage)\n",
    "    final_score = (sem_score * 0.5 + coverage * 0.5) * 100\n",
    "    \n",
    "    results.append({\n",
    "        \"question_index\": idx+1,\n",
    "        \"user_answer\": user_text,\n",
    "        \"reference_question\": ref.get(\"question\", \"\"),\n",
    "        \"similarity_percent\": round(sem_score*100, 2),\n",
    "        \"keyword_coverage_percent\": round(coverage*100, 2),\n",
    "        \"final_score_percent\": round(final_score, 2)\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Print results\n",
    "for r in results:\n",
    "    print(\"\\nQuestion\", r[\"question_index\"])\n",
    "    print(\"Reference Question:\", r.get(\"reference_question\", \"N/A\"))\n",
    "    print(\"User Answer:\", r.get(\"user_answer\", \"\"))\n",
    "    print(\"Semantic Similarity:\", r.get(\"similarity_percent\"), \"%\")\n",
    "    print(\"Keyword Coverage:\", r.get(\"keyword_coverage_percent\"), \"%\")\n",
    "    print(\"Final Combined Score:\", r.get(\"final_score_percent\"), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663e669-babb-4bf3-874e-1fe1ff286e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
